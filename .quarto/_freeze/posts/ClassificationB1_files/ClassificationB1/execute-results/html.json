{
  "hash": "de273112d729d3f8d36977b0489ff255",
  "result": {
    "markdown": "---\ntitle: 'Classification Theorems: Unveiling Patterns in Data'\nimage: heartphoto.jpg\ndate: '2023-11-28'\ncategories:\n  - Analysis\n  - Visualization\n---\n\n# Introduction\n\nHeart disease remains a significant health concern globally. Machine learning models have shown promise in predicting the likelihood of heart disease based on various medical attributes. In this document, we explore the application of machine learning in predicting heart disease using a dataset containing several health-related features.\n\n## Dataset Overview\n\nThe dataset used in this analysis contains several medical attributes that could potentially correlate with heart disease. These attributes include:\n\n- Age\n- Sex\n- Chest pain type (cp)\n- Resting blood pressure (trtbps)\n- Serum cholesterol (chol)\n- Fasting blood sugar (fbs)\n- Resting electrocardiographic results (restecg)\n- Maximum heart rate achieved (thalachh)\n- Exercise-induced angina (exng)\n- ST depression induced by exercise relative to rest (oldpeak)\n- Slope of the peak exercise ST segment (slp)\n- Number of major vessels (0-3) colored by fluoroscopy (caa)\n- Thalassemia (thall)\n\nThe last column 'output' represents the presence of heart disease, where '1' indicates the presence and '0' indicates the absence of heart disease.\n\n\n## Methodology\n\nTwo classification algorithms, namely decision tree and random forest, were employed on this dataset to predict the likelihood of heart disease based on the provided features. Both algorithms were trained, validated, and evaluated for their predictive performance.\n\n\n## Data Preprocessing\n\nTo prepare the dataset for modeling, data cleaning techniques were implemented. This involved handling missing values, addressing outliers, and encoding categorical variables if necessary.\n\n## Data Splitting and Model Implementation of Decision Tree\n\nThe dataset was split into training and testing sets to train the machine learning models and assess their performance. The decision tree algorithm was implemented on the training set to predict the likelihood of heart disease based on the provided features.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndf = pd.read_csv(\"heart.csv\")\ndf.head(10)\ndf.tail(10)\ndf.shape\ndf.info()\ndf.describe()\ndf.isna().sum()\nsns.heatmap(df.isna())\ndf.duplicated().sum()\ndisplay(df.drop_duplicates(inplace=True)) \ndf.duplicated().sum()\ndf['output'].value_counts()\ndf.hist(figsize=(15,10))\nplt.show()\ncorr= df.corr()\nsns.heatmap(corr.rank(axis='columns'),cmap='Blues',annot=True)\nX = df.drop('output',axis='columns')\ny= df['output']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nDT = DecisionTreeClassifier()\nDT.fit(X_train,y_train)\ny_prediction = DT.predict(X_test)\ntree_train_acc = round(accuracy_score(y_train,DT.predict(X_train))*100,2)\ntree_test_acc = round(accuracy_score(y_test,y_prediction)*100,2)\nprint('Accuracy = ' , tree_test_acc,' %')\nprint(classification_report(\n\n    y_test,\n    DT.predict(X_test)\n\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 303 entries, 0 to 302\nData columns (total 14 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       303 non-null    int64  \n 1   sex       303 non-null    int64  \n 2   cp        303 non-null    int64  \n 3   trtbps    303 non-null    int64  \n 4   chol      303 non-null    int64  \n 5   fbs       303 non-null    int64  \n 6   restecg   303 non-null    int64  \n 7   thalachh  303 non-null    int64  \n 8   exng      303 non-null    int64  \n 9   oldpeak   303 non-null    float64\n 10  slp       303 non-null    int64  \n 11  caa       303 non-null    int64  \n 12  thall     303 non-null    int64  \n 13  output    303 non-null    int64  \ndtypes: float64(1), int64(13)\nmemory usage: 33.3 KB\nAccuracy =  78.69  %\n              precision    recall  f1-score   support\n\n           0       0.74      0.86      0.79        29\n           1       0.85      0.72      0.78        32\n\n    accuracy                           0.79        61\n   macro avg       0.79      0.79      0.79        61\nweighted avg       0.80      0.79      0.79        61\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nNone\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ClassificationB1_files/figure-html/cell-2-output-3.png){width=574 height=462}\n:::\n\n::: {.cell-output .cell-output-display}\n![](ClassificationB1_files/figure-html/cell-2-output-4.png){width=1172 height=801}\n:::\n\n::: {.cell-output .cell-output-display}\n![](ClassificationB1_files/figure-html/cell-2-output-5.png){width=574 height=461}\n:::\n:::\n\n\n## Random Forest Model Implementation\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nRF = RandomForestClassifier()\nRF.fit(X_train,y_train)\ny_prediction = RF.predict(X_test)\nrdm_train_acc = round(accuracy_score(y_train,RF.predict(X_train))*100,2)\nrdm_test_acc = round(accuracy_score(y_test,y_prediction)*100,2)\nprint('Accuracy = ' , rdm_test_acc,' %')\nprint(classification_report(\n\n    y_test,\n    RF.predict(X_test)\n\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy =  85.25  %\n              precision    recall  f1-score   support\n\n           0       0.83      0.86      0.85        29\n           1       0.87      0.84      0.86        32\n\n    accuracy                           0.85        61\n   macro avg       0.85      0.85      0.85        61\nweighted avg       0.85      0.85      0.85        61\n\n```\n:::\n:::\n\n\n# Results\n\nThe performance of the decision tree and random forest models was assessed based on various evaluation metrics such as accuracy, precision, recall, and F1-score. The comparison of these models' performance provides insights into their effectiveness in predicting heart disease.\n\nThe accuracy achieved by the models:\n- Decision Tree: 78.69%\n- Random Forest: 86.89%\n\nThis indicates that the random forest algorithm outperforms the decision tree in terms of accuracy in predicting heart disease.\n\n## Conclusion\n\nMachine learning models, specifically decision tree and random forest, demonstrate potential in predicting heart disease based on various medical indicators. The evaluation of these models sheds light on their strengths and weaknesses in accurately identifying the presence of heart disease.\n\n",
    "supporting": [
      "ClassificationB1_files"
    ],
    "filters": [],
    "includes": {}
  }
}