{"title":"Classification Theorems: Unveiling Patterns in Data","markdown":{"yaml":{"title":"Classification Theorems: Unveiling Patterns in Data","image":"heartphoto.jpg","date":"2023-11-28","categories":["Analysis","Visualization"],"jupyter":"python3"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\nHeart disease remains a significant health concern globally. Machine learning models have shown promise in predicting the likelihood of heart disease based on various medical attributes. In this document, we explore the application of machine learning in predicting heart disease using a dataset containing several health-related features.\n\n## Dataset Overview\n\nThe dataset used in this analysis contains several medical attributes that could potentially correlate with heart disease. These attributes include:\n\n- Age\n- Sex\n- Chest pain type (cp)\n- Resting blood pressure (trtbps)\n- Serum cholesterol (chol)\n- Fasting blood sugar (fbs)\n- Resting electrocardiographic results (restecg)\n- Maximum heart rate achieved (thalachh)\n- Exercise-induced angina (exng)\n- ST depression induced by exercise relative to rest (oldpeak)\n- Slope of the peak exercise ST segment (slp)\n- Number of major vessels (0-3) colored by fluoroscopy (caa)\n- Thalassemia (thall)\n\nThe last column 'output' represents the presence of heart disease, where '1' indicates the presence and '0' indicates the absence of heart disease.\n\n\n## Methodology\n\nTwo classification algorithms, namely decision tree and random forest, were employed on this dataset to predict the likelihood of heart disease based on the provided features. Both algorithms were trained, validated, and evaluated for their predictive performance.\n\n\n## Data Preprocessing\n\nTo prepare the dataset for modeling, data cleaning techniques were implemented. This involved handling missing values, addressing outliers, and encoding categorical variables if necessary.\n\n## Data Splitting and Model Implementation of Decision Tree\n\nThe dataset was split into training and testing sets to train the machine learning models and assess their performance. The decision tree algorithm was implemented on the training set to predict the likelihood of heart disease based on the provided features.\n\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndf = pd.read_csv(\"heart.csv\")\ndf.head(10)\ndf.tail(10)\ndf.shape\ndf.info()\ndf.describe()\ndf.isna().sum()\nsns.heatmap(df.isna())\ndf.duplicated().sum()\ndisplay(df.drop_duplicates(inplace=True)) \ndf.duplicated().sum()\ndf['output'].value_counts()\ndf.hist(figsize=(15,10))\nplt.show()\ncorr= df.corr()\nsns.heatmap(corr.rank(axis='columns'),cmap='Blues',annot=True)\nX = df.drop('output',axis='columns')\ny= df['output']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nDT = DecisionTreeClassifier()\nDT.fit(X_train,y_train)\ny_prediction = DT.predict(X_test)\ntree_train_acc = round(accuracy_score(y_train,DT.predict(X_train))*100,2)\ntree_test_acc = round(accuracy_score(y_test,y_prediction)*100,2)\nprint('Accuracy = ' , tree_test_acc,' %')\nprint(classification_report(\n\n    y_test,\n    DT.predict(X_test)\n\n))\n```\n\n\n## Random Forest Model Implementation\n\n\n\n\n\n```{python}\nRF = RandomForestClassifier()\nRF.fit(X_train,y_train)\ny_prediction = RF.predict(X_test)\nrdm_train_acc = round(accuracy_score(y_train,RF.predict(X_train))*100,2)\nrdm_test_acc = round(accuracy_score(y_test,y_prediction)*100,2)\nprint('Accuracy = ' , rdm_test_acc,' %')\nprint(classification_report(\n\n    y_test,\n    RF.predict(X_test)\n\n))\n```\n\n# Results\n\nThe performance of the decision tree and random forest models was assessed based on various evaluation metrics such as accuracy, precision, recall, and F1-score. The comparison of these models' performance provides insights into their effectiveness in predicting heart disease.\n\nThe accuracy achieved by the models:\n- Decision Tree: 78.69%\n- Random Forest: 86.89%\n\nThis indicates that the random forest algorithm outperforms the decision tree in terms of accuracy in predicting heart disease.\n\n## Conclusion\n\nMachine learning models, specifically decision tree and random forest, demonstrate potential in predicting heart disease based on various medical indicators. The evaluation of these models sheds light on their strengths and weaknesses in accurately identifying the presence of heart disease.\n\n","srcMarkdownNoYaml":"\n# Introduction\n\nHeart disease remains a significant health concern globally. Machine learning models have shown promise in predicting the likelihood of heart disease based on various medical attributes. In this document, we explore the application of machine learning in predicting heart disease using a dataset containing several health-related features.\n\n## Dataset Overview\n\nThe dataset used in this analysis contains several medical attributes that could potentially correlate with heart disease. These attributes include:\n\n- Age\n- Sex\n- Chest pain type (cp)\n- Resting blood pressure (trtbps)\n- Serum cholesterol (chol)\n- Fasting blood sugar (fbs)\n- Resting electrocardiographic results (restecg)\n- Maximum heart rate achieved (thalachh)\n- Exercise-induced angina (exng)\n- ST depression induced by exercise relative to rest (oldpeak)\n- Slope of the peak exercise ST segment (slp)\n- Number of major vessels (0-3) colored by fluoroscopy (caa)\n- Thalassemia (thall)\n\nThe last column 'output' represents the presence of heart disease, where '1' indicates the presence and '0' indicates the absence of heart disease.\n\n\n## Methodology\n\nTwo classification algorithms, namely decision tree and random forest, were employed on this dataset to predict the likelihood of heart disease based on the provided features. Both algorithms were trained, validated, and evaluated for their predictive performance.\n\n\n## Data Preprocessing\n\nTo prepare the dataset for modeling, data cleaning techniques were implemented. This involved handling missing values, addressing outliers, and encoding categorical variables if necessary.\n\n## Data Splitting and Model Implementation of Decision Tree\n\nThe dataset was split into training and testing sets to train the machine learning models and assess their performance. The decision tree algorithm was implemented on the training set to predict the likelihood of heart disease based on the provided features.\n\n\n```{python}\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndf = pd.read_csv(\"heart.csv\")\ndf.head(10)\ndf.tail(10)\ndf.shape\ndf.info()\ndf.describe()\ndf.isna().sum()\nsns.heatmap(df.isna())\ndf.duplicated().sum()\ndisplay(df.drop_duplicates(inplace=True)) \ndf.duplicated().sum()\ndf['output'].value_counts()\ndf.hist(figsize=(15,10))\nplt.show()\ncorr= df.corr()\nsns.heatmap(corr.rank(axis='columns'),cmap='Blues',annot=True)\nX = df.drop('output',axis='columns')\ny= df['output']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nDT = DecisionTreeClassifier()\nDT.fit(X_train,y_train)\ny_prediction = DT.predict(X_test)\ntree_train_acc = round(accuracy_score(y_train,DT.predict(X_train))*100,2)\ntree_test_acc = round(accuracy_score(y_test,y_prediction)*100,2)\nprint('Accuracy = ' , tree_test_acc,' %')\nprint(classification_report(\n\n    y_test,\n    DT.predict(X_test)\n\n))\n```\n\n\n## Random Forest Model Implementation\n\n\n\n\n\n```{python}\nRF = RandomForestClassifier()\nRF.fit(X_train,y_train)\ny_prediction = RF.predict(X_test)\nrdm_train_acc = round(accuracy_score(y_train,RF.predict(X_train))*100,2)\nrdm_test_acc = round(accuracy_score(y_test,y_prediction)*100,2)\nprint('Accuracy = ' , rdm_test_acc,' %')\nprint(classification_report(\n\n    y_test,\n    RF.predict(X_test)\n\n))\n```\n\n# Results\n\nThe performance of the decision tree and random forest models was assessed based on various evaluation metrics such as accuracy, precision, recall, and F1-score. The comparison of these models' performance provides insights into their effectiveness in predicting heart disease.\n\nThe accuracy achieved by the models:\n- Decision Tree: 78.69%\n- Random Forest: 86.89%\n\nThis indicates that the random forest algorithm outperforms the decision tree in terms of accuracy in predicting heart disease.\n\n## Conclusion\n\nMachine learning models, specifically decision tree and random forest, demonstrate potential in predicting heart disease based on various medical indicators. The evaluation of these models sheds light on their strengths and weaknesses in accurately identifying the presence of heart disease.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"ClassificationB1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title-block-banner":true,"title":"Classification Theorems: Unveiling Patterns in Data","image":"heartphoto.jpg","date":"2023-11-28","categories":["Analysis","Visualization"],"jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}