{
  "hash": "efe31de610e204335056e8d048fb4a37",
  "result": {
    "markdown": "---\ntitle: Linear and Non- linear regression\nformat:\n  html:\n    code-fold: true\n---\n\nThis blog-post provides an overview and comparison of linear and non-linear regression techniques, common methods used in predictive modeling.\n\n## Linear Regression\n\nLinear regression is a statistical method used for modeling the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the input variables and the output.\n\n### Algorithm Explanation\n\n1. **Model Representation**: Linear regression represents the relationship between the dependent variable `Y` and the independent variable(s) `X` using a linear equation: `Y = b0 + b1*X + ε`, where `b0` is the intercept, `b1` is the coefficient, and `ε` is the error term.\n2. **Objective**: The model aims to find the best-fitting line that minimizes the sum of the squared differences between actual and predicted values (Ordinary Least Squares method).\n3. **Model Evaluation**: Common evaluation metrics include R-squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). These metrics assess how well the model fits the data.\n\n### Use Cases\n\nLinear regression is suitable when the relationship between variables is linear. It's commonly used in scenarios such as predicting sales based on advertising expenditure, predicting housing prices based on area, etc.\n\n## Non-linear Regression\n\nNon-linear regression is used when the relationship between the dependent and independent variables is not linear. It models complex relationships by fitting a curve instead of a straight line.\n\n### Algorithm Explanation\n\n1. **Model Representation**: Non-linear regression uses non-linear equations to represent the relationship between variables. Examples include quadratic, exponential, logarithmic, and sigmoid functions.\n2. **Objective**: Similar to linear regression, the aim is to minimize the difference between actual and predicted values but using non-linear functions to fit the data.\n3. **Model Evaluation**: Evaluation metrics remain similar to linear regression, with R-squared, MSE, and RMSE being commonly used.\n\n### Use Cases\n\nNon-linear regression is applicable when the relationship between variables is better represented by curves or other non-linear functions. Examples include modeling population growth, predicting disease spread, etc.\n\n## Conclusion\n\nLinear regression is suitable for simpler relationships between variables, assuming linearity, while non-linear regression is more flexible, capturing complex patterns in the data. The choice between linear and non-linear regression depends on the nature of the data and the underlying relationship between variables.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Load the dataset\nfile_path = 'car data.csv'\ndata = pd.read_csv(file_path)\n\n# Selecting independent and dependent variables\nX = data[['Year', 'Present_Price', 'Kms_Driven', 'Owner']]  # Features\ny = data['Selling_Price']  # Target variable\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Linear Regression\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train, y_train)\n\n# Prediction on the test set\ny_pred = linear_reg.predict(X_test)\n\n# Calculating RMSE (Root Mean Squared Error)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nprint('RMSE:', rmse)\n\n# Visualization\nplt.scatter(y_test, y_pred)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\nplt.xlabel('Actual Selling Price')\nplt.ylabel('Predicted Selling Price')\nplt.title('Linear Regression (RMSE: {:.2f})'.format(rmse))\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE: 2.0304088376313625\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](B3LinearAndNonlinearRegression_files/figure-html/cell-2-output-2.png){width=585 height=449}\n:::\n:::\n\n\n## Code Explanation\n\n### Importing Libraries\nThe code begins by importing necessary libraries: Pandas, NumPy, train_test_split, LinearRegression from scikit-learn, and matplotlib.pyplot.\n\n### Loading and Preparing Data\n- Loads the dataset 'car data.csv' into a Pandas DataFrame named `data`.\n- Selects independent variables ('Year', 'Present_Price', 'Kms_Driven') as `X` and the dependent variable ('Selling_Price') as `y`.\n- Splits the dataset into training and testing sets using train_test_split, allocating 80% for training and 20% for testing.\n\n### Building the Linear Regression Model\n- Initializes a Linear Regression model using `LinearRegression()` from scikit-learn.\n- Fits the model on the training data (`X_train`, `y_train`), learning the coefficients for the linear equation.\n\n### Model Evaluation\n- Uses the trained model to make predictions on the test data (`X_test`).\n- Generates a scatter plot comparing predicted selling prices with actual selling prices to visualize the model's performance.\n\n## Conclusion\n\nThe linear regression model successfully predicts selling prices of cars based on features like year, present price, and kilometers driven. The scatter plot visually represents the alignment between predicted and actual selling prices, allowing assessment of the model's performance.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nfile_path = 'car data.csv'\ndata = pd.read_csv(file_path)\n\n# Select relevant columns for analysis\ncolumns_for_regression = ['Year', 'Present_Price', 'Kms_Driven', 'Fuel_Type', 'Seller_Type', 'Transmission', 'Selling_Price']\ndata = data[columns_for_regression]\n\n# Encoding categorical variables\nlabel_encoder = LabelEncoder()\ndata['Fuel_Type'] = label_encoder.fit_transform(data['Fuel_Type'])\ndata['Seller_Type'] = label_encoder.fit_transform(data['Seller_Type'])\ndata['Transmission'] = label_encoder.fit_transform(data['Transmission'])\n\n# Independent variables (X) and dependent variable (y)\nX = data.drop('Selling_Price', axis=1)\ny = data['Selling_Price']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Polynomial regression\npoly = PolynomialFeatures(degree=2)  # Choose the degree of the polynomial\nX_train_poly = poly.fit_transform(X_train)\nX_test_poly = poly.transform(X_test)\n\n# Fitting the polynomial regression model\nmodel = LinearRegression()\nmodel.fit(X_train_poly, y_train)\n\n# Predictions on the test set\ny_pred = model.predict(X_test_poly)\n\n# Evaluation - Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\n# Visualization - Actual vs Predicted\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Selling Price')\nplt.ylabel('Predicted Selling Price')\nplt.title('Actual vs Predicted Selling Price (non-linear Regression)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 0.7269897857594595\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](B3LinearAndNonlinearRegression_files/figure-html/cell-3-output-2.png){width=585 height=449}\n:::\n:::\n\n\n# Polynomial Regression for Predicting Car Selling Price\n\nThis code snippet demonstrates polynomial regression to predict the selling price of cars based on various features.\n\n## Code Explanation\n\nThe code performs the following steps:\n\n### Data Loading and Preprocessing\n\n- Loads the 'car data.csv' dataset and selects relevant columns ('Year', 'Present_Price', 'Kms_Driven', 'Fuel_Type', 'Seller_Type', 'Transmission', 'Selling_Price').\n- Encodes categorical variables ('Fuel_Type', 'Seller_Type', 'Transmission') using LabelEncoder to convert them into numerical format for analysis.\n\n### Data Splitting\n\n- Splits the dataset into independent variables (X) and the dependent variable (y) representing features and the target variable ('Selling_Price') respectively.\n- Further splits the data into training and testing sets using `train_test_split`.\n\n### Polynomial Regression\n\n- Utilizes PolynomialFeatures from scikit-learn to generate polynomial and interaction features. The degree of the polynomial is set to 2 for demonstration purposes.\n- Transforms the features into non-linear form and fits the Linear Regression model.\n\n### Model Fitting and Prediction\n\n- Fits the non-linear regression model using the training data.\n- Makes predictions on the test set using the trained model.\n\n### Evaluation and Visualization\n\n- Calculates the Mean Squared Error (MSE) to evaluate the model's performance.\n- Generates a scatter plot to visualize the relationship between the actual and predicted selling prices.\n\n## Conclusion\n\nThis code showcases how non-linear regression can be used to predict car selling prices based on various features. Adjusting the degree of the polynomial or exploring different non-linear regression models can potentially capture more complex relationships in the data.\n\n",
    "supporting": [
      "B3LinearAndNonlinearRegression_files"
    ],
    "filters": [],
    "includes": {}
  }
}