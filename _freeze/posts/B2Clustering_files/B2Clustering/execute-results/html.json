{
  "hash": "877ead46bf7cea1c5b8860e9a769fc34",
  "result": {
    "markdown": "---\ntitle: 'Clustering Decoded: Understanding Patterns in Complex Data'\nformat:\n  html:\n    code-fold: true\n---\n\n# What is Clustering?\n\nClustering is a technique used in machine learning and data analysis to group similar data points together based on certain characteristics or features. The primary goal of clustering is to segregate data into different groups or clusters, where data points within the same cluster are more similar to each other compared to those in other clusters.The process of clustering involves the following key aspects like Grouping Data , Unsupervised Learning , Similarity or Distance Measurement .\n\n# Types of Clustering Algorithms\n\n## K-Means Clustering\n\nK-Means is one of the most popular clustering algorithms used for partitioning data into K clusters.\nIt aims to group data points into K clusters, minimizing the variance within each cluster.\nThe algorithm iteratively assigns each data point to the nearest centroid (representative point) and recalculates the centroids until convergence.\nIt's a centroid-based algorithm and requires the number of clusters (K) as input.\n\n## Hierarchical Clustering\n\nHierarchical clustering builds a hierarchy of clusters, either by creating a bottom-up approach (agglomerative) or a top-down approach (divisive).\nAgglomerative clustering starts by considering each data point as a single cluster and then merges the most similar clusters iteratively until there's only one cluster containing all the data points.\nDivisive clustering begins with one cluster that encompasses all data points and then divides it into smaller clusters until each cluster consists of only one data point.\nThe resulting hierarchy can be represented as a dendrogram.\n\n\n## Density-Based Clustering (DBSCAN)\nDBSCAN is a density-based clustering algorithm that identifies clusters based on the density of data points.\nIt groups together data points that are closely packed, considering points within a specified radius as part of the same cluster.\nIt doesn't require the number of clusters as an input and can identify clusters of arbitrary shapes.\nDBSCAN can also identify noise (outliers) as points that do not belong to any cluster.\n## Gaussian Mixture Models (GMM)\n\nGMM assumes that the data points are generated from a mixture of several Gaussian distributions.\nIt identifies clusters by estimating the parameters (mean, variance, and weight) of these Gaussian distributions.\nUnlike K-Means, GMM allows data points to belong to multiple clusters with different probabilities, making it more flexible for complex data distributions.\n\n# Let us take an example \n\nThe code code provides an initial exploration and clustering of movie-related data, allowing for visual interpretation of how movies are grouped based on popularity and voting metrics. Further analysis and domain-specific insights can be drawn from the identified clusters to understand different movie categories or audience preferences.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nfile_path = 'Top_rated_movies1.csv'\ndata = pd.read_csv(file_path)\n\n# Extracting relevant columns for clustering\ncolumns_for_clustering = ['popularity', 'vote_average', 'vote_count']\nclustering_data = data[columns_for_clustering]\n\n# Handling missing values if any\nclustering_data.fillna(0, inplace=True)  # Filling missing values with 0, but handle them appropriately\n\n# Standardize the data to have mean=0 and variance=1\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(clustering_data)\n\n# Choosing the number of clusters (in this case, let's assume 5 clusters)\nnum_clusters = 5\n\n# Apply K-Means clustering\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\ndata['cluster_label'] = kmeans.fit_predict(scaled_data)\n\n# Visualizing the clusters (2D plot for two features)\nplt.scatter(data['popularity'], data['vote_average'], c=data['cluster_label'], cmap='viridis')\nplt.xlabel('Popularity')\nplt.ylabel('Vote Average')\nplt.title('K-Means Clustering')\nplt.show()\n\n# Displaying cluster centers\ncluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\ncluster_centers_df = pd.DataFrame(cluster_centers, columns=columns_for_clustering)\nprint(\"Cluster Centers:\")\nprint(cluster_centers_df)\n\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/gs/1d1cby_x735fdcbz03nrn9g80000gn/T/ipykernel_29024/1807398126.py:15: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/Users/shailshah/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](B2Clustering_files/figure-html/cell-2-output-2.png){width=576 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster Centers:\n    popularity  vote_average    vote_count\n0    25.553087      5.931799   1137.382767\n1    82.265458      7.355368  11369.133333\n2    28.767516      7.180168   1468.184962\n3  1566.076750      7.589000   2415.750000\n4   420.881270      7.243378   3366.864865\n```\n:::\n:::\n\n\n# Explanation\n\n## Data Preparation:\nRelevant columns ('popularity', 'vote_average', 'vote_count') were selected for clustering. Missing values, if any, were filled with zeros. The data was standardized using `StandardScaler` to ensure mean=0 and variance=1 for each feature.\n\n## Clustering:\nK-Means clustering was applied with five clusters (`num_clusters = 5`) on the standardized data. Each data point was assigned a 'cluster_label' based on its similarity to the cluster centers.\n\n## Visualization:\nThe clustering results were visualized using a scatter plot. The x-axis represents 'popularity', the y-axis represents 'vote_average', and data points were colored according to their assigned cluster labels. This visualization helps in understanding how data points are distributed among different clusters based on these two features.\n\n## Cluster Centers:\nThe code also printed the cluster centers (mean values of 'popularity', 'vote_average', 'vote_count') after transforming them back to the original scale.\n\n## Insights:\nThe analysis allows for the identification of groups or clusters of movies based on their popularity, average votes, and vote counts. Understanding the characteristics of each cluster (e.g., movies with high popularity but lower vote averages or vice versa) might provide insights into audience preferences or critical acclaim.\n\n",
    "supporting": [
      "B2Clustering_files"
    ],
    "filters": [],
    "includes": {}
  }
}